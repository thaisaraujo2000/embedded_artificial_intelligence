{"cells":[{"cell_type":"markdown","metadata":{"id":"j4RpK9pawQzP"},"source":["# Your First Image Classifier: Using CNN to Classify Images\n","# Pre-processing"]},{"cell_type":"markdown","metadata":{"id":"vbuzxq-b90j-"},"source":["The purpose of this dataset is to correctly classify an image as containing a dog, cat, or panda.\n","Containing only 3,000 images, the Animals dataset is meant to be another **introductory** dataset\n","that we can quickly train a CNN model and obtain a comparative results with the previous KNN model.\n","\n","Let's take the following steps:\n","\n","1. Fetch Data (reuse of the previous project)\n","2. Pre-processing\n","3. Clean data\n","\n","<center><img width=\"900\" src=\"https://drive.google.com/uc?export=view&id=1haMB_Zt6Et9q9sPHxfuR4g3FT5QRXlTI\"></center>\n"]},{"cell_type":"markdown","metadata":{"id":"ApYpc47MFOYi"},"source":["## Step 01: Setup"]},{"cell_type":"markdown","metadata":{"id":"ULBka-lQFJW9"},"source":["Start out by installing the experiment tracking library and setting up your free W&B account:\n","\n","\n","*   **pip install wandb** – Install the W&B library\n","*   **import wandb** – Import the wandb library\n","*   **wandb login** – Login to your W&B account so you can log all your metrics in one place"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"vWnFIWPuFXej","executionInfo":{"status":"ok","timestamp":1665932193519,"user_tz":180,"elapsed":3046,"user":{"displayName":"THAÍS MEDEIROS","userId":"04939613717659582607"}}},"outputs":[],"source":["!pip install wandb -qU"]},{"cell_type":"markdown","metadata":{"id":"wcrOk6pURp50"},"source":["### Import Packages"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"VJaCNlDDRz6d","executionInfo":{"status":"ok","timestamp":1665932196539,"user_tz":180,"elapsed":3037,"user":{"displayName":"THAÍS MEDEIROS","userId":"04939613717659582607"}}},"outputs":[],"source":["# import the necessary packages\n","from imutils import paths\n","import logging\n","import os\n","import cv2\n","import numpy as np\n","import joblib\n","import tensorflow as tf\n","import wandb"]},{"cell_type":"code","source":["wandb.login()"],"metadata":{"id":"GaqfBz4Ol3p8","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1665932199293,"user_tz":180,"elapsed":2770,"user":{"displayName":"THAÍS MEDEIROS","userId":"04939613717659582607"}},"outputId":"b1c8e571-3831-4514-bbd3-81a21f2178e1"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stderr","text":["ERROR:wandb.jupyter:Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mthaisaraujom\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":3}]},{"cell_type":"code","execution_count":4,"metadata":{"id":"qkIkAWcL-4x7","executionInfo":{"status":"ok","timestamp":1665932199300,"user_tz":180,"elapsed":37,"user":{"displayName":"THAÍS MEDEIROS","userId":"04939613717659582607"}}},"outputs":[],"source":["# configure logging\n","# reference for a logging obj\n","logger = logging.getLogger()\n","\n","# set level of logging\n","logger.setLevel(logging.INFO)\n","\n","# create handlers\n","c_handler = logging.StreamHandler()\n","c_format = logging.Formatter(fmt=\"%(asctime)s %(message)s\",datefmt='%d-%m-%Y %H:%M:%S')\n","c_handler.setFormatter(c_format)\n","\n","# add handler to the logger\n","logger.handlers[0] = c_handler"]},{"cell_type":"markdown","metadata":{"id":"m-XgvGlGx-n_"},"source":["## Step 02: Fetch Data"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"KlLArYonw4pJ","executionInfo":{"status":"ok","timestamp":1665932199307,"user_tz":180,"elapsed":43,"user":{"displayName":"THAÍS MEDEIROS","userId":"04939613717659582607"}}},"outputs":[],"source":["# since we are using Jupyter Notebooks we can replace our argument\n","# parsing code with *hard coded* arguments and values\n","args = {\n","\t\"dataset\": \"animals\",\n","  \"project_name\": \"first_image_classifier\",\n","  \"artifact_name\": \"animals_raw_data:latest\",\n","}"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"r3kCOus2wzbw","colab":{"base_uri":"https://localhost:8080/","height":139},"executionInfo":{"status":"ok","timestamp":1665932201769,"user_tz":180,"elapsed":2505,"user":{"displayName":"THAÍS MEDEIROS","userId":"04939613717659582607"}},"outputId":"d930503f-a523-4381-fefd-5304a530704e"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Tracking run with wandb version 0.13.4"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Run data is saved locally in <code>/content/wandb/run-20221016_145637-nv0htuny</code>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Syncing run <strong><a href=\"https://wandb.ai/thaisaraujom/first_image_classifier/runs/nv0htuny\" target=\"_blank\">vocal-tree-10</a></strong> to <a href=\"https://wandb.ai/thaisaraujom/first_image_classifier\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact animals_raw_data:latest, 187.97MB. 3000 files... \n","\u001b[34m\u001b[1mwandb\u001b[0m:   3000 of 3000 files downloaded.  \n","Done. 0:0:0.4\n","16-10-2022 14:56:40 Path: ./artifacts/animals_raw_data:v0\n"]}],"source":["# open the W&B project created in the Fetch step\n","run = wandb.init(entity=\"thaisaraujom\",project=args[\"project_name\"], job_type=\"preprocessing\")\n","\n","# download the raw data from W&B\n","raw_data = run.use_artifact(args[\"artifact_name\"])\n","data_dir = raw_data.download()\n","logger.info(\"Path: {}\".format(data_dir))"]},{"cell_type":"code","source":["run.finish()"],"metadata":{"id":"r-aLVJrYFtSV","colab":{"base_uri":"https://localhost:8080/","height":150,"referenced_widgets":["0361285803d14b29bd306e214d60be1f","87ec6132928f44ffa1951b24a9ef8542","2142e6ac8b3742bdb1eb4b34e98edadb","5a91a0bd98de4e84ba6b58b6e935cc62","a09887e12223439288e82af3969bdbc2","c96a72787a9b4363b5193e646542c25d","9dc2bf11297c480282a5c36ac9f4aaee","20dafe50a9f74c4196324e107c80e8e5"]},"executionInfo":{"status":"ok","timestamp":1665932205759,"user_tz":180,"elapsed":4015,"user":{"displayName":"THAÍS MEDEIROS","userId":"04939613717659582607"}},"outputId":"f01858c9-ba04-46a3-d045-22e66f8290df"},"execution_count":7,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["VBox(children=(Label(value='0.000 MB of 0.000 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0361285803d14b29bd306e214d60be1f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Synced <strong style=\"color:#cdcd00\">vocal-tree-10</strong>: <a href=\"https://wandb.ai/thaisaraujom/first_image_classifier/runs/nv0htuny\" target=\"_blank\">https://wandb.ai/thaisaraujom/first_image_classifier/runs/nv0htuny</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Find logs at: <code>./wandb/run-20221016_145637-nv0htuny/logs</code>"]},"metadata":{}}]},{"cell_type":"markdown","metadata":{"id":"z4hRGKj23tJQ"},"source":["## Step 03 - Clean Data"]},{"cell_type":"markdown","source":["### Project Config."],"metadata":{"id":"9RpRG_c2JNjd"}},{"cell_type":"code","source":["data_dir"],"metadata":{"id":"GcZcyjj0GCC8","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1665932205772,"user_tz":180,"elapsed":66,"user":{"displayName":"THAÍS MEDEIROS","userId":"04939613717659582607"}},"outputId":"abf2274f-6796-4aa0-d84f-cf91b15f1ef5"},"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'./artifacts/animals_raw_data:v0'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":8}]},{"cell_type":"code","execution_count":9,"metadata":{"id":"_rlVvuII53sR","executionInfo":{"status":"ok","timestamp":1665932205776,"user_tz":180,"elapsed":64,"user":{"displayName":"THAÍS MEDEIROS","userId":"04939613717659582607"}}},"outputs":[],"source":["# since we are using Jupyter Notebooks we can replace our argument\n","# parsing code with *hard coded* arguments and values\n","args = {\n","\t\"features\": \"clean_features\",\n","  \"target\": \"labels\",\n","  \"project_name\": \"cnn_classifier\"\n","}"]},{"cell_type":"code","source":["# open the W&B project created in the Fetch step\n","run = wandb.init(entity=\"thaisaraujom\",project=args[\"project_name\"], job_type=\"preprocessing\")"],"metadata":{"id":"YvF_qIB5NlKm","colab":{"base_uri":"https://localhost:8080/","height":69},"executionInfo":{"status":"ok","timestamp":1665932206898,"user_tz":180,"elapsed":1183,"user":{"displayName":"THAÍS MEDEIROS","userId":"04939613717659582607"}},"outputId":"2f6d78df-7a56-4c20-fb56-b9e1b548d683"},"execution_count":10,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Tracking run with wandb version 0.13.4"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Run data is saved locally in <code>/content/wandb/run-20221016_145644-1z3p2nrb</code>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Syncing run <strong><a href=\"https://wandb.ai/thaisaraujom/cnn_classifier/runs/1z3p2nrb\" target=\"_blank\">deep-yogurt-8</a></strong> to <a href=\"https://wandb.ai/thaisaraujom/cnn_classifier\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"]},"metadata":{}}]},{"cell_type":"markdown","source":["### Loader and Preprocessing Classes"],"metadata":{"id":"O_5dUXUKJVyQ"}},{"cell_type":"markdown","source":["Source code based on **Rosebrock, Adrian. Deep Learning For Computer vision with Python, 2019** [link](https://pyimagesearch.com/deep-learning-computer-vision-python-book/)"],"metadata":{"id":"gciJIGOgDHOg"}},{"cell_type":"code","execution_count":11,"metadata":{"id":"zEeK-4xH69L7","executionInfo":{"status":"ok","timestamp":1665932206901,"user_tz":180,"elapsed":24,"user":{"displayName":"THAÍS MEDEIROS","userId":"04939613717659582607"}}},"outputs":[],"source":["# \n","# a basic simple preprocessor that resize a image\n","#\n","class SimplePreprocessor:\n","\tdef __init__(self, width, height, inter=cv2.INTER_AREA):\n","\t\t# store the target image width, height, and interpolation\n","\t\t# method used when resizing\n","\t\tself.width = width\n","\t\tself.height = height\n","\t\tself.inter = inter\n","\n","\tdef preprocess(self, image):\n","\t\t# resize the image to a fixed size, ignoring the aspect\n","\t\t# ratio\n","\t\treturn cv2.resize(image, (self.width, self.height),interpolation=self.inter)"]},{"cell_type":"code","source":["#\n","# Rearrange the dimension of an image and return a numpy array\n","# Default dimension is (heigh, width, channel)\n","#\n","class ImageToArrayPreprocessor:\n","\tdef __init__(self, dataFormat=None):\n","\t\t# store the image data format\n","\t\tself.dataFormat = dataFormat\n","\n","\tdef preprocess(self, image):\n","\t\t# apply the Keras utility function that correctly rearranges\n","\t\t# the dimensions of the image\n","\t\treturn tf.keras.utils.img_to_array(image, data_format=self.dataFormat)"],"metadata":{"id":"t7lYi1pAIica","executionInfo":{"status":"ok","timestamp":1665932206903,"user_tz":180,"elapsed":22,"user":{"displayName":"THAÍS MEDEIROS","userId":"04939613717659582607"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","execution_count":13,"metadata":{"id":"x_YLPvoP7OjX","executionInfo":{"status":"ok","timestamp":1665932206905,"user_tz":180,"elapsed":22,"user":{"displayName":"THAÍS MEDEIROS","userId":"04939613717659582607"}}},"outputs":[],"source":["# Building an image loader\n","class SimpleDatasetLoader:\n","  def __init__(self, preprocessors=None, logger=None):\n","\t\t# store the image preprocessor\n","    self.preprocessors = preprocessors\n","    self.logger = logger\n","\n","\t\t# if the preprocessors are None, initialize them as an\n","\t\t# empty list\n","    if self.preprocessors is None:\n","      self.preprocessors = []\n","\n","  def load(self, imagePaths, verbose=-1):\n","\t\t# initialize the list of features and labels\n","    data = []\n","    labels = []\n","\n","\t\t# loop over the input images\n","    for (i, imagePath) in enumerate(imagePaths):\n","\t\t\t# load the image and extract the class label assuming\n","\t\t\t# that our path has the following format:\n","\t\t\t# /path/to/dataset/{class}/{image}.jpg\n","\t\t\t# e.g \"img example: ./artifacts/animals_raw_data:v0/dogs/dogs_00892.jpg\"\n","\t\t\t# imagePath.split(os.path.sep)[-2] will return \"dogs\"\n","      image = cv2.imread(imagePath)\n","      label = imagePath.split(os.path.sep)[-2]\n","\n","      # check to see if our preprocessors are not None\n","      if self.preprocessors is not None:\n","\t\t\t\t# loop over the preprocessors and apply each to\n","\t\t\t\t# the image\n","        for p in self.preprocessors:\n","          image = p.preprocess(image)\n","\n","\t\t\t# treat our processed image as a \"feature vector\"\n","\t\t\t# by updating the data list followed by the labels\n","      data.append(image)\n","      labels.append(label)\n","   \n","\t\t\t# show an update every `verbose` images\n","      if verbose > 0 and i > 0 and (i + 1) % verbose == 0:\n","        logger.info(\"[INFO] processed {}/{}\".format(i + 1,len(imagePaths)))\n","\n","\t\t# return a tuple of the data and labels\n","    return (np.array(data), np.array(labels))"]},{"cell_type":"markdown","source":["### Cleaning"],"metadata":{"id":"2KmT8j1SMfNq"}},{"cell_type":"code","execution_count":14,"metadata":{"id":"OC8kcWO07wxB","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1665932216833,"user_tz":180,"elapsed":9948,"user":{"displayName":"THAÍS MEDEIROS","userId":"04939613717659582607"}},"outputId":"8f8169d5-b30e-4785-c458-a618a13e1b12"},"outputs":[{"output_type":"stream","name":"stderr","text":["16-10-2022 14:56:45 [INFO] preprocessing images...\n","16-10-2022 14:56:46 [INFO] processed 500/3000\n","16-10-2022 14:56:47 [INFO] processed 1000/3000\n","16-10-2022 14:56:48 [INFO] processed 1500/3000\n","16-10-2022 14:56:49 [INFO] processed 2000/3000\n","16-10-2022 14:56:52 [INFO] processed 2500/3000\n","16-10-2022 14:56:54 [INFO] processed 3000/3000\n","16-10-2022 14:56:54 [INFO] features matrix: 70.3MB\n","16-10-2022 14:56:54 [INFO] labels vector: 0.1MB\n","16-10-2022 14:56:54 [INFO] features shape: (3000, 32, 32, 3), labels shape: (3000,)\n"]}],"source":["# grab the list of images that we'll be describing\n","logger.info(\"[INFO] preprocessing images...\")\n","imagePaths = list(paths.list_images(data_dir))\n","\n","# initialize the image preprocessors\n","sp = SimplePreprocessor(32, 32)\n","iap = ImageToArrayPreprocessor()\n","\n","# load the dataset from disk then scale the raw pixel intensities\n","# to the range [0, 1]\n","sdl = SimpleDatasetLoader(preprocessors=[sp, iap])\n","(data, labels) = sdl.load(imagePaths, verbose=500)\n","data = data.astype(\"float\") / 255.0\n","\n","# show some information on memory consumption of the images\n","logger.info(\"[INFO] features matrix: {:.1f}MB\".format(data.nbytes / (1024 * 1024)))\n","logger.info(\"[INFO] labels vector: {:.1f}MB\".format(labels.nbytes / (1024 * 1024)))\n","logger.info(\"[INFO] features shape: {}, labels shape: {}\".format(data.shape,labels.shape))"]},{"cell_type":"markdown","source":["### Dump the artifacts to disk and upload to W&B"],"metadata":{"id":"yPexHIfcNHm_"}},{"cell_type":"code","execution_count":15,"metadata":{"id":"o9KK4Jii8pJe","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1665932216835,"user_tz":180,"elapsed":68,"user":{"displayName":"THAÍS MEDEIROS","userId":"04939613717659582607"}},"outputId":"556efe21-9905-4b20-c92d-3510dd29b6b0"},"outputs":[{"output_type":"stream","name":"stderr","text":["16-10-2022 14:56:54 Dumping the clean data artifacts to disk\n"]}],"source":["# Save the feature artifacts using joblib\n","joblib.dump(data, args[\"features\"])\n","\n","# Save the target using joblib\n","joblib.dump(labels, args[\"target\"])\n","\n","logger.info(\"Dumping the clean data artifacts to disk\")"]},{"cell_type":"code","execution_count":16,"metadata":{"id":"bnnmdpK9BLD2","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1665932217243,"user_tz":180,"elapsed":466,"user":{"displayName":"THAÍS MEDEIROS","userId":"04939613717659582607"}},"outputId":"7bfbdd5f-6595-4ac7-8362-59ecb3e2d3f5"},"outputs":[{"output_type":"stream","name":"stderr","text":["16-10-2022 14:56:54 Logging clean data artifact\n"]},{"output_type":"execute_result","data":{"text/plain":["<wandb.sdk.wandb_artifacts.Artifact at 0x7f5146fd4bd0>"]},"metadata":{},"execution_count":16}],"source":["# clean data artifact\n","artifact = wandb.Artifact(args[\"features\"],\n","                          type=\"CLEAN_DATA\",\n","                          description=\"A json file representing the clean features data\"\n","                          )\n","\n","logger.info(\"Logging clean data artifact\")\n","artifact.add_file(args[\"features\"])\n","run.log_artifact(artifact)"]},{"cell_type":"code","execution_count":17,"metadata":{"id":"2RxuZTgACWVw","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1665932217247,"user_tz":180,"elapsed":29,"user":{"displayName":"THAÍS MEDEIROS","userId":"04939613717659582607"}},"outputId":"85ca9f6f-8ccc-43f9-c7c1-b4ad7137d373"},"outputs":[{"output_type":"stream","name":"stderr","text":["16-10-2022 14:56:55 Logging clean target artifact\n"]},{"output_type":"execute_result","data":{"text/plain":["<wandb.sdk.wandb_artifacts.Artifact at 0x7f5147044290>"]},"metadata":{},"execution_count":17}],"source":["# clean label artifact\n","artifact = wandb.Artifact(args[\"target\"],\n","                          type=\"CLEAN_DATA\",\n","                          description=\"A json file representing the clean target\"\n","                          )\n","\n","logger.info(\"Logging clean target artifact\")\n","artifact.add_file(args[\"target\"])\n","run.log_artifact(artifact)"]},{"cell_type":"code","execution_count":18,"metadata":{"id":"49Ivx0SQCkhT","colab":{"base_uri":"https://localhost:8080/","height":86},"executionInfo":{"status":"ok","timestamp":1665932223570,"user_tz":180,"elapsed":6345,"user":{"displayName":"THAÍS MEDEIROS","userId":"04939613717659582607"}},"outputId":"9cfb831a-53a8-4aff-a639-cede7ccd62fb"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Synced <strong style=\"color:#cdcd00\">deep-yogurt-8</strong>: <a href=\"https://wandb.ai/thaisaraujom/cnn_classifier/runs/1z3p2nrb\" target=\"_blank\">https://wandb.ai/thaisaraujom/cnn_classifier/runs/1z3p2nrb</a><br/>Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Find logs at: <code>./wandb/run-20221016_145644-1z3p2nrb/logs</code>"]},"metadata":{}}],"source":["run.finish()"]}],"metadata":{"colab":{"collapsed_sections":[],"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"0361285803d14b29bd306e214d60be1f":{"model_module":"@jupyter-widgets/controls","model_name":"VBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"VBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"VBoxView","box_style":"","children":["IPY_MODEL_87ec6132928f44ffa1951b24a9ef8542","IPY_MODEL_2142e6ac8b3742bdb1eb4b34e98edadb"],"layout":"IPY_MODEL_5a91a0bd98de4e84ba6b58b6e935cc62"}},"87ec6132928f44ffa1951b24a9ef8542":{"model_module":"@jupyter-widgets/controls","model_name":"LabelModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"LabelModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"LabelView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a09887e12223439288e82af3969bdbc2","placeholder":"​","style":"IPY_MODEL_c96a72787a9b4363b5193e646542c25d","value":"0.000 MB of 0.000 MB uploaded (0.000 MB deduped)\r"}},"2142e6ac8b3742bdb1eb4b34e98edadb":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_9dc2bf11297c480282a5c36ac9f4aaee","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_20dafe50a9f74c4196324e107c80e8e5","value":1}},"5a91a0bd98de4e84ba6b58b6e935cc62":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a09887e12223439288e82af3969bdbc2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c96a72787a9b4363b5193e646542c25d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9dc2bf11297c480282a5c36ac9f4aaee":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"20dafe50a9f74c4196324e107c80e8e5":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}}}}},"nbformat":4,"nbformat_minor":0}